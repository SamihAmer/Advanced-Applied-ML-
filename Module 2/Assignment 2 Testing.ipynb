{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version= 2.7.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 72\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f'PyTorch version= {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/samihamer/Desktop/Advanced Applied AI Summer 2025/Module 2/MNIST/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import struct\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "            images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 28, 28, 1)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 60000, HxW= 28x28\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = load_mnist(DATA_DIR, kind='train')\n",
    "print(f'N= {X_tr.shape[0]}, HxW= {X_tr.shape[1]}x{X_tr.shape[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 60000, HxW= 28x28\n",
      "N= 10000, HxW= 28x28\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = load_mnist(DATA_DIR, kind='train')\n",
    "print(f'N= {X_tr.shape[0]}, HxW= {X_tr.shape[1]}x{X_tr.shape[2]}')\n",
    "\n",
    "X_ts, y_ts = load_mnist(DATA_DIR, kind='t10k')\n",
    "print(f'N= {X_ts.shape[0]}, HxW= {X_ts.shape[1]}x{X_ts.shape[2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to work with Conv2D - Batch, Chn, H, W\n",
    "X_tr = X_tr.reshape(X_tr.shape[0], 1, X_tr.shape[1], X_tr.shape[2])\n",
    "X_ts = X_ts.reshape(X_ts.shape[0], 1, X_ts.shape[1], X_ts.shape[2])\n",
    "\n",
    "IMG_CHANNEL= 1  # color channel\n",
    "MLP_HIDDEN= 64  # Hidden layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, epochs=10, eta=0.001, minibatch_size=5000, seed=0):\n",
    "        super().__init__()\n",
    "        self.random = np.random.RandomState(seed)  # shuffle mini batches\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "        self.optimizer = None\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.model = None\n",
    "\n",
    "    def init_layers(self, _M:int, _K:int) -> None:\n",
    "        # data structure\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(IMG_CHANNEL, MLP_HIDDEN, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(MLP_HIDDEN),\n",
    "\n",
    "            nn.Conv2d(MLP_HIDDEN, MLP_HIDDEN*2, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(start_dim=1),\n",
    "\n",
    "            # 4*4 is computed in the above cell\n",
    "            nn.Linear(MLP_HIDDEN*2 * 4*4, 1024),  # 1024 arbitrary\n",
    "            nn.BatchNorm1d(1024),\n",
    "\n",
    "            nn.Linear(1024, _K),\n",
    "        ).to(device)\n",
    "\n",
    "    def predict(self, _X):\n",
    "        _X = torch.FloatTensor(_X).to(device, non_blocking=True)\n",
    "        assert self.model is not None\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(_X)\n",
    "        self.model.train()\n",
    "        # probs = nn.functional.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, _X):\n",
    "        _X = torch.FloatTensor(_X).to(device, non_blocking=True)\n",
    "        assert self.model is not None\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(_X)\n",
    "        self.model.train()\n",
    "        probs = nn.functional.softmax(logits, dim=1)\n",
    "        # preds = torch.argmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def fit(self, _X_train, _y_train, info=False):\n",
    "        import sys\n",
    "        n_features= _X_train.shape[1]\n",
    "        n_output= np.unique(_y_train).shape[0]  # number of class labels\n",
    "\n",
    "        _X_train = torch.FloatTensor(_X_train).to(device, non_blocking=True)\n",
    "        _y_train = torch.LongTensor(_y_train).to(device, non_blocking=True).long()\n",
    "\n",
    "        self.init_layers(n_features, n_output)\n",
    "        self.optimizer = torch.optim.Rprop(self.model.parameters(), lr=self.eta)  # connect model to optimizer\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            indices = np.arange(_X_train.shape[0])\n",
    "            self.random.shuffle(indices)  # shuffle the data each epoch\n",
    "\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                net_out = self.model(_X_train[batch_idx])\n",
    "\n",
    "                loss = self.loss_func(net_out, _y_train[batch_idx])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if info:\n",
    "                    sys.stderr.write(f\"\\r{e+1:03d} Loss: {loss.item():6.5f}\")\n",
    "                    sys.stderr.flush()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "013 Loss: 0.01740"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.991\n",
      "CPU times: user 15min 55s, sys: 7min 14s, total: 23min 10s\n",
      "Wall time: 15min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlp = PyTorchMLP(epochs=13).to(device)\n",
    "mlp.fit(X_tr, y_tr, info=True)\n",
    "\n",
    "# Testing dataset\n",
    "y_pred = mlp.predict(X_ts)\n",
    "\n",
    "print(f'Accuracy= {np.sum(y_pred==y_ts)/len(X_ts):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899b82f0a4a74e3ab02113199bbc0428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 3000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m example \u001b[38;5;241m=\u001b[39m X_ts[example_idx]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Get explanation\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m explanation \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(\n\u001b[1;32m     33\u001b[0m     example,\n\u001b[1;32m     34\u001b[0m     predict_fn,\n\u001b[1;32m     35\u001b[0m     segmentation_fn\u001b[38;5;241m=\u001b[39msegmenter,\n\u001b[1;32m     36\u001b[0m     top_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     37\u001b[0m     hide_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     38\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Plot original image and explanation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, digit \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lime/lime_image.py:216\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    212\u001b[0m     ret_exp\u001b[38;5;241m.\u001b[39mtop_labels\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m top:\n\u001b[1;32m    214\u001b[0m     (ret_exp\u001b[38;5;241m.\u001b[39mintercept[label],\n\u001b[1;32m    215\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mlocal_exp[label],\n\u001b[0;32m--> 216\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mscore, ret_exp\u001b[38;5;241m.\u001b[39mlocal_pred) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mexplain_instance_with_data(\n\u001b[1;32m    217\u001b[0m         data, labels, distances, label, num_features,\n\u001b[1;32m    218\u001b[0m         model_regressor\u001b[38;5;241m=\u001b[39mmodel_regressor,\n\u001b[1;32m    219\u001b[0m         feature_selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret_exp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lime/lime_base.py:183\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[1;32m    181\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_fn(distances)\n\u001b[1;32m    182\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m neighborhood_labels[:, label]\n\u001b[0;32m--> 183\u001b[0m used_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection(neighborhood_data,\n\u001b[1;32m    184\u001b[0m                                        labels_column,\n\u001b[1;32m    185\u001b[0m                                        weights,\n\u001b[1;32m    186\u001b[0m                                        num_features,\n\u001b[1;32m    187\u001b[0m                                        feature_selection)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_regressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     model_regressor \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m                             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lime/lime_base.py:134\u001b[0m, in \u001b[0;36mLimeBase.feature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     n_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighest_weights\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection(data, labels, weights,\n\u001b[1;32m    135\u001b[0m                               num_features, n_method)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lime/lime_base.py:80\u001b[0m, in \u001b[0;36mLimeBase.feature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighest_weights\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     clf \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m                 random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m---> 80\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(data, labels, sample_weight\u001b[38;5;241m=\u001b[39mweights)\n\u001b[1;32m     82\u001b[0m     coef \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(data):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:1123\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m _accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[0;32m-> 1123\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1124\u001b[0m     X,\n\u001b[1;32m   1125\u001b[0m     y,\n\u001b[1;32m   1126\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m_accept_sparse,\n\u001b[1;32m   1127\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[1;32m   1128\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1129\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1130\u001b[0m )\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 3000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create LIME explainer\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "# Define segmentation algorithm\n",
    "segmenter = SegmentationAlgorithm('quickshift', kernel_size=4, max_dist=200, ratio=0.2)\n",
    "\n",
    "# Function to preprocess images for LIME\n",
    "def preprocess_for_lime(images):\n",
    "    return images.reshape(-1, 28, 28)\n",
    "\n",
    "# Function to get model predictions for LIME\n",
    "def predict_fn(images):\n",
    "    # Reshape images to match model input\n",
    "    images = images.reshape(-1, 1, 28, 28)\n",
    "    # Get probabilities for all classes\n",
    "    probs = mlp.predict_proba(images)\n",
    "    # Return probabilities for the predicted class only\n",
    "    return probs[:, np.argmax(probs[0])]\n",
    "\n",
    "# Get explanations for each digit class\n",
    "plt.figure(figsize=(15, 10))\n",
    "for digit in range(10):\n",
    "    # Find an example of the current digit\n",
    "    digit_indices = np.where(y_ts == digit)[0]\n",
    "    if len(digit_indices) > 0:\n",
    "        example_idx = digit_indices[0]\n",
    "        example = X_ts[example_idx].reshape(28, 28)\n",
    "        \n",
    "        # Get explanation\n",
    "        explanation = explainer.explain_instance(\n",
    "            example,\n",
    "            predict_fn,\n",
    "            segmentation_fn=segmenter,\n",
    "            top_labels=1,\n",
    "            hide_color=0,\n",
    "            num_samples=1000\n",
    "        )\n",
    "        \n",
    "        # Plot original image and explanation\n",
    "        plt.subplot(2, 5, digit + 1)\n",
    "        temp, mask = explanation.get_image_and_mask(\n",
    "            explanation.top_labels[0],\n",
    "            positive_only=True,\n",
    "            num_features=5,\n",
    "            hide_rest=True\n",
    "        )\n",
    "        plt.imshow(mask, cmap='RdBu', alpha=0.7)\n",
    "        plt.imshow(example, cmap='gray', alpha=0.5)\n",
    "        plt.title(f'Digit {digit}')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
